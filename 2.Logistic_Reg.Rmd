---
title: 'Lasso and Elastic-Net Regularized for Logistic Regression'
author: "Chunyi Wu and Andrzej Galecki"
date: "`r as.character(Sys.Date(), format = '%A %B %d, %Y')`"
output:
  rmdformats::readthedown:
    lightbox: true
    use_bookdown: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="#>")
```

```{r, data, echo=FALSE, message=FALSE, warning=FALSE}
if (!require('pacman')) install.packages('pacman', repos = "http://cran.us.r-project.org")
library(pacman)

pacman::p_load(
  readxl,       # load excel file
  glmnet,       # Lasso and Elastic-Net Regularized Generalized Linear Models
  rmdformats,   # rmd formats
  rmarkdown,    # rmarkdown
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics 
  janitor,      # adding totals and percents to tables
  flextable,    # format the output table
  gtsummary,    # calculate summary statistics and format the results
  sjPlot,       # correlation matrix
  purrr,        # enhances R functional programming (FP) toolki 
  tidyr,        #Tools to help to create tidy data
  ggplot2,      #Plot the results
  glmnetUtils,  #Glmnet models for multiple alpha
  coefplot      # Plotting Model Coefficients
  )
```

```{r utilsag}
if (!require('utilsag')) devtools::install_github("agalecki/utilsag", ref = "version-0.07")
library(utilsag)
```

# Introduction

In this report we consider two logistic regression models for the  `status` (0/1) variable:

* M1: Contains 21 proteins as candidate covariates
* M2 :Contains 21 proteins and Baseline HbA1c, log10(ACR), BL_eGFR, SEX, and AGE_TL(Baseline Age) as candidate covariates

We will refer to Model M1 as "unadjusted" and to Model M2 as "adjusted" model.

Note: This logistic regression model does not take into account the duration of the follow- up time and it is included for illustrative purposes only.

# glmnet for Model M1 ($\alpha=0$)

## Data preparation

Data are prepared for "unadjusted" logistic regression (Model M1)

```{r, prepare-data, echo=TRUE, message=FALSE, warning=FALSE}
#read the data that is stored under the data folder

scrambled<-readRDS(file = "./Data/scrambled.rds")

scrambled <- scrambled  %>% mutate(log10_DU_ACR=log10(DU_ACR))

dim(scrambled)

#select variables for unadjusted logistic regression model
data <- scrambled  %>% select("KIM1.npx","SYND1.npx","IL.1RT1.npx",
 "WFDC2.npx", "CD27.npx", "TNFRSF10A.npx","LAYN.npx","PVRL4.npx",
 "EDA2R.npx","TNFRSF4.npx", "GFR_alpha_1_npx","TNF.R1.npx","PI3_npx", 
 "EFNA4.npx","TNF.R2.npx" ,"DLL1.npx", "TNFRSF6B.npx", "CD160.npx",
 "EPHA2.npx","RELT.npx","LTBR.npx","time","status") 

#drop any records with NA
data_drop_na <- data %>% drop_na()    

#Total sample size
nrow(data_drop_na)

#includes all proteins after excluding missing data
x <- data.matrix(data_drop_na[,c("KIM1.npx","SYND1.npx","IL.1RT1.npx","WFDC2.npx",
        "CD27.npx", "TNFRSF10A.npx","LAYN.npx","PVRL4.npx","EDA2R.npx",
        "TNFRSF4.npx","GFR_alpha_1_npx","TNF.R1.npx","PI3_npx", "EFNA4.npx",
        "TNF.R2.npx" ,"DLL1.npx" ,"TNFRSF6B.npx", "CD160.npx","EPHA2.npx",
        "RELT.npx","LTBR.npx")])
dim(x)
colnames(x)

#select follow-up time and status
y <- data.matrix(data_drop_na[,c("status")])
table(y)
```

## Model M1 fit $\alpha=0$

* By default mixing (hyper) parameter $\alpha$ is equal to $1$ (lasso)
* In this example we assume $\alpha$ is equal to $0$ (ridge regression)


```{r, Unadjusted, echo=TRUE, message=FALSE, warning=FALSE}
#fit the unadjusted logistic regression model
pfit <- glmnet(x, y, family = "binomial", alpha=0)
class(pfit)
```

## Extracting info from Model M1

### Tidy results

```{r mytidy-pfit}
myglance(pfit)
mytidy(pfit)
```


...

###  Model M1 Coefficients Profile Plots

* (Standardized) coefficients plotted versus ${\ell}_1$-norm`(using plot()`function:

```{r, coefficient-profile-plot1, echo=TRUE, message=FALSE, warning=FALSE}
plot(pfit, label = TRUE) 
```

"Each curve corresponds to a variable. It shows the path of its coefficient 
against the ${\ell}_1$-norm of the whole 
coefficient vector as $\lambda$ varies. The axis above indicates 
the number of nonzero coefficients at the current $\lambda$,
which is the effective degrees of freedom (df) for the lasso."

source: ("https://glmnet.stanford.edu/articles/glmnet.html")

* (Standardized) coefficients plotted versus $\log(\lambda)$ using `coefplot::coefpath`:

```{r, coefficient-profile-plot12, echo=TRUE, message=FALSE, warning=FALSE}
coefpath(pfit) 
```

# cv.glmnet for M1 ($\alpha=0$)

## C-V for M1 ($\alpha=0$)

* Unadjusted Cross-Validated Logistic Regression Model Coefficient Plot

```{r, Unadjusted_cv_plot, echo=TRUE, message=FALSE, warning=FALSE}
#fit the cross-validated model
cvfit1 <- cv.glmnet(x, y, family = "binomial", alpha=0)
plot(cvfit1, label = TRUE) 
```

"This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves
along the $\lambda$ sequence (error bars). Two special values along the $\lambda$ sequence are indicated by the 
vertical dotted lines. `lambda.min` is the value of $\lambda$ that gives minimum mean cross-validated error, 
while `lambda.1se` is the value of $\lambda$ that gives the most regularized model such that the cross-validated 
error is within one standard error of the minimum."

source: ("https://glmnet.stanford.edu/articles/glmnet.html")

## Extract info from C-V (M1)

### minimal lambda value

* Unadjusted Cross-Validated Logistic Regression Coefficients using `lambda.min` value 
* get the minimal lambda value (value of lambda that gives minimum cvm)

```{r, Unadjusted_cv_min, echo=TRUE, message=FALSE, warning=FALSE}
cvfit1$lambda.min
coef(cvfit1, s = "lambda.min")
```

### lambda.lse

* Unadjusted Cross-Validated Logistic Regression Coefficients using `lambda.lse`
* `lambda.min` is the largest value of lambda such that error is within 1 standard error of the minimum.
# Cross-validated coefficients of the Logistic Regression Model using the lambda.lse

```{r, Unadjusted_cv_lse, echo=TRUE, message=FALSE, warning=FALSE}
cvfit1$lambda.1se
coef(cvfit1)
```

# glmnetUtils (M1)

* Glmnet models for multiple alpha 
* We use cross-validation to tune hyperparameter $\alpha$.
* The idea of "explicitly control the fold" is implemented in `glmnetUtils` package
* The cva.glmnet function does simultaneous cross-validation for both the $\alpha$ and $\lambda$ parameters in an elastic net model.

source: (https://glmnet.stanford.edu/articles/glmnet.html)

```{r glmnetUtils}

set.seed(46)
alphv <- seq(0, 1, len = 11)^3
cva_pfit1 <- cva.glmnet(x=x,y=y,family = "binomial", alpha = alphv)
minlossplot(cva_pfit1)
```

Extract the best (hyper)parameters from cva.glmnet object

```{r get_best_params}
# Extract the best parameters from cva.glmnet object.
get_model_params <- function(fit) {
  alpha <- fit$alpha
  lambdaMin <- sapply(fit$modlist, `[[`, "lambda.min")
  lambdaSE <- sapply(fit$modlist, `[[`, "lambda.1se")
  error <- sapply(fit$modlist, function(mod) {min(mod$cvm)})
  best <- which.min(error)
  data.frame(alpha = alpha[best], lambdaMin = lambdaMin[best],
             lambdaSE = lambdaSE[best], eror = error[best])
}

get_model_params(cva_pfit1)	
```

Logistic regression coefficients for Model M1 using the best combination of (hyper)parameters

```{r get_best_coef}
coef(cva_pfit1, s = get_model_params(cva_pfit1)$lambdaMin,
        alpha=get_model_params(cva_pfit1)$alpha)
```

Save `cva_pfit1` object for post-processing

```{r save-cva_pfit1}
save(cva_pfit1, file = "./save/21Logistic-Reg1-save1.Rdata")
```


# glmnet for Model M2 ($\alpha=0$)


* Adjusted Logistic Regression Model M2(adjusting for Baseline HbA1c, Log10(ACR), eGFR, Sex, and Age)

## Data preparation

```{r, Adjusted, echo=TRUE, message=FALSE, warning=FALSE}

#select variables for adjusted model
data2 <- scrambled  %>% select("KIM1.npx","SYND1.npx","IL.1RT1.npx","WFDC2.npx",
         "CD27.npx", "TNFRSF10A.npx","LAYN.npx","PVRL4.npx","EDA2R.npx",
         "TNFRSF4.npx", "GFR_alpha_1_npx","TNF.R1.npx","PI3_npx", 
         "EFNA4.npx","TNF.R2.npx" , "DLL1.npx" ,"TNFRSF6B.npx", 
         "CD160.npx","EPHA2.npx","RELT.npx","LTBR.npx",
         "time","status","BL_eGFR","B_HBA1C_PRC","log10_DU_ACR","SEX","AGE_TL") 
dim(data2)         

#drop any records with NA
data_drop_na2 <- data2 %>% drop_na()    

#Total sample size
nrow(data_drop_na2)

#includes all proteins after excluding missing data
x2 <- data.matrix(data_drop_na2[,c("KIM1.npx","SYND1.npx","IL.1RT1.npx",
        "WFDC2.npx", "CD27.npx", "TNFRSF10A.npx","LAYN.npx","PVRL4.npx",
        "EDA2R.npx","TNFRSF4.npx", "GFR_alpha_1_npx","TNF.R1.npx","PI3_npx",
        "EFNA4.npx","TNF.R2.npx","DLL1.npx" ,"TNFRSF6B.npx", "CD160.npx",
        "EPHA2.npx","RELT.npx","LTBR.npx","B_HBA1C_PRC","log10_DU_ACR","BL_eGFR","SEX","AGE_TL")])

#select follow-up time and status
y2 <- data.matrix(data_drop_na2[,c("status")])
```

## Model M2 fit ($\alpha=0$)

```{r glmnet-pfit2, echo=TRUE, message=FALSE, warning=FALSE}
#fit the adjusted logistic regression model
pfit2 <- glmnet(x2, y2, family = "binomial", alpha=0)

```

## Extracting info from Model M2

### Coefficient plots

* Adjusted Logistic Regression Model Coefficient Plot

```{r, Adjusted_plot, echo=TRUE, message=FALSE, warning=FALSE}
plot(pfit2, label = TRUE) 
```

# cv.glmnet for M2 ($\alpha=0$)


## C-V for M2 ($\alpha=0$)

* Adjusted Cross-Validated Logistic Regression Model Coefficient Plot

```{r, Adjusted_cv_plot, echo=TRUE, message=FALSE, warning=FALSE}
#fit the cross-validatedmodel
cvfit2 <- cv.glmnet(x2, y2, family = "binomial", alpha=0)

plot(cvfit2, label = TRUE) 
```


## Extract info from C-V (M2)
### Coefficients for selected $\lambda$

* Adjusted Cross-Validated Logistic Regressionn Coefficients using `lambda.min` value

```{r, Adjusted_cv_min, echo=TRUE, message=FALSE, warning=FALSE}
#get the minimal lambda value (value of lambda that gives minimum cvm)
(l_min <- cvfit2$lambda.min)

# Express on natural log scale
log(l_min)
# Cross-validated Logistic Regression Model using the minimal lambda
coef(cvfit2, s = "lambda.min")
```

* Adjusted Cross-Validated Logistic Regression Coefficients using `lambda.1se`
* largest value of lambda such that error is within 1 standard error of the minimum.

```{r, Adjusted_cv_lse, echo=TRUE, message=FALSE, warning=FALSE}
(l_1se <- cvfit2$lambda.1se)

# Express on natural log scale
log(l_1se)

coef(cvfit2)
```

# glmnetUtils (M2)

* Glmnet models for multiple alpha 
* We use cross-validation to tune hyperparameter $\alpha$.
* The idea of "explicitly control the fold" is implemented in `glmnetUtils` package
* The cva.glmnet function does simultaneous cross-validation for both the $\alpha$ and $\lambda$ parameters in an elastic net model.

source: (https://glmnet.stanford.edu/articles/glmnet.html)

```{r glmnetUtils_M2}

set.seed(46)
alphv <- seq(0, 1, len = 11)^3
cva_pfit2 <- cva.glmnet(x=x2,y=y2,family = "binomial", alpha = alphv)
minlossplot(cva_pfit2)
```

Extract the best (hyper)parameters from cva.glmnet object

```{r get_best_params_M2}
# Extract the best parameters from cva.glmnet object.
get_model_params <- function(fit) {
  alpha <- fit$alpha
  lambdaMin <- sapply(fit$modlist, `[[`, "lambda.min")
  lambdaSE <- sapply(fit$modlist, `[[`, "lambda.1se")
  error <- sapply(fit$modlist, function(mod) {min(mod$cvm)})
  best <- which.min(error)
  data.frame(alpha = alpha[best], lambdaMin = lambdaMin[best],
             lambdaSE = lambdaSE[best], eror = error[best])
}

get_model_params(cva_pfit2)	
```

Logistic regression coefficients for Model M2 using the best combination of (hyper)parameters

```{r get_best_coef_M2}
coef(cva_pfit2, s = get_model_params(cva_pfit2)$lambdaMin,
        alpha=get_model_params(cva_pfit2)$alpha)
```

* Save `cva_pfit2` object for post-processing

```{r save-cva_pfit1_M2}
save(cva_pfit2, file = "./save/21Logistic-Reg1-save2.Rdata")
```
