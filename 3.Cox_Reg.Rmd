---
title: 'Lasso and Elastic-Net Regularized for Cox Regression'
author: "Chunyi Wu and Andrzej Galecki"
date: "`r as.character(Sys.Date(), format = '%A %B %d, %Y')`"
output:
  rmdformats::readthedown:
    lightbox: true
    use_bookdown: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment="#>")
```

```{r, data, echo=FALSE, message=FALSE, warning=FALSE}
if (!require('pacman')) install.packages('pacman', repos = "http://cran.us.r-project.org")
library(pacman)

pacman::p_load(
  readxl,       # load excel file
  glmnet,       # Lasso and Elastic-Net Regularized Generalized Linear Models
  rmdformats,   # rmd formats
  rmarkdown,    # rmarkdown
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics 
  janitor,      # adding totals and percents to tables
  flextable,    # format the output table
  gtsummary,    # calculate summary statistics and format the results
  sjPlot,       # correlation matrix
  purrr,        # enhances R functional programming (FP) toolki 
  tidyr,        #Tools to help to create tidy data
  ggplot2,      #Plot the results
  glmnetUtils,  #Glmnet models for multiple alpha
  coefplot,     # Plotting Model Coefficients
  survival,     #survival model 
  tidymodels,   #for modeling and machine learning using tidyverse principles
  survivalROC   #survivalROC
  )
```
```{r utilsag}
if (!require('utilsag')) devtools::install_github("agalecki/utilsag") #, ref = "version-0.1.1")
library(utilsag)
```

# Introduction

In this report we consider three Cox regression models for the  time-to-event `time` variable. Originally this 
variable was named `FU_TIME`.  

`status` (0/1) variable, originally named `CASE_CONTROL`, is coded 0 for Controls (i.e., ESKD event has not occurred), and 1 for Cases (i.e., event has occurred)

* M0: Contains Baseline HbA1c, log10(ACR), BL_eGFR, SEX, and AGE_TL(Baseline Age) as candidate predictors

* M1: Contains 21 proteins as candidate predictors

* M2 :Contains 21 proteins and Baseline HbA1c, log10(ACR), BL_eGFR, SEX, and AGE_TL(Baseline Age) as candidate predictors

We will refer to Model M1 as "unadjusted" and to Model M2 as "adjusted" model.


## Data prep

Data are prepared for Cox regression. Note that input data called `scrambled` was created
by executing Descriptive Statistics script       

```{r, prepare-data, echo=TRUE, message=FALSE, warning=FALSE}
#read the data that is stored under the data folder

scrambled <- readRDS(file = "./Data/scrambled.rds")

scrambled <- scrambled  %>% mutate(log10_DU_ACR = log10(DU_ACR))  %>% filter(time>0)

dim(scrambled) # Number of rows and columns in the input data

# variables used in Cox regression model
prot_npx <- c("KIM1.npx","SYND1.npx","IL.1RT1.npx",   "WFDC2.npx", "CD27.npx",
              "TNFRSF10A.npx","LAYN.npx","PVRL4.npx", "EDA2R.npx","TNFRSF4.npx",
              "GFR_alpha_1_npx","TNF.R1.npx","PI3_npx", "EFNA4.npx","TNF.R2.npx" ,
              "DLL1.npx", "TNFRSF6B.npx", "CD160.npx", "EPHA2.npx","RELT.npx",
              "LTBR.npx") 
surv_vars <-  c("time", "status")
clin_vars <- c("BL_eGFR","B_HBA1C_PRC","log10_DU_ACR","SEX","AGE_TL")
```

???- M0-starts

# glmnet for Model M0 ($\alpha=0$)


## Data for M0

Data are prepared for Cox regression (Model M0).

```{r create-data-drop-na0}
vars0 <- c(surv_vars, clin_vars)
 
data0 <- scrambled  %>% select(all_of(vars0)) 

#drop any records with NA
data_drop_na0 <- data0 %>% drop_na()    

# Total sample size and number of columns
dim(data_drop_na0)

#includes all proteins after excluding missing data
x0 <- data.matrix(data_drop_na0[, clin_vars])
dim(x0)
colnames(x0)  # 5 clinical predictors

# time and status
tx <- data.matrix(data_drop_na0[,c("time","status")])
ty    <- tx[, "time"]
stx <- tx[,"status"]
ySurv0 <- survival::Surv(ty, stx)   
table(data_drop_na0$status) # 0-censored, 1-observed time-to-ESKD
```

## Model M0 fit $\alpha=0$

* By default mixing (hyper) parameter $\alpha$ is equal to $1$ (lasso)
* In this example we arbitrarily assume $\alpha$ is equal to $0$ (ridge regression)


```{r Unadjusted-M0, echo = TRUE, message= FALSE, warning=FALSE}
#fit the unadjusted cox regression model
pfit0 <- glmnet(x0, ySurv0, family = "cox", alpha=0)
class(pfit0)
```
          
## Extracting info from Model M0 fit



###  Model M0 Coefficients Profile Plots

* (Standardized) coefficients plotted versus ${\ell}_1$-norm (using `plot()` function:

```{r, coefficient-profile-plot0, echo=TRUE, message=FALSE}
plot(pfit0, label = TRUE) 
```

"Each curve corresponds to a variable. It shows the path of its coefficient 
against the ${\ell}_1$-norm of the whole 
coefficient vector as $\lambda$ varies. The axis above indicates 
the number of nonzero coefficients at the current $\lambda$,
which is the effective degrees of freedom (df) for the lasso."

source: ("https://glmnet.stanford.edu/articles/glmnet.html")

# cv.glmnet for M0 ($\alpha=0$)

## C-V for M0 ($\alpha=0$)

* Cross-Validated Cox Regression Model Coefficient Plot for Model M0

```{r, Unadjusted_cv_plot0, echo=TRUE, message=FALSE}
set.seed(1)
#fit the cross-validated model
cvfit0 <- cv.glmnet(x0, ySurv0, family = "cox", alpha=0)
plot(cvfit0, label = TRUE) 
```

"This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves
along the $\lambda$ sequence (error bars). Two special values along the $\lambda$ sequence are indicated by the 
vertical dotted lines. `lambda.min` is the value of $\lambda$ that gives minimum mean cross-validated error, 
while `lambda.1se` is the value of $\lambda$ that gives the most regularized model such that the cross-validated 
error is within one standard error of the minimum."

source: ("https://glmnet.stanford.edu/articles/glmnet.html")

## Extract info from C-V (M0)

### minimal lambda value

* Unadjusted Cross-Validated Cox Regression Coefficients using `lambda.min` value 
* get the minimal lambda value (value of lambda that gives minimum cvm)

```{r, Unadjusted-M0-cv_min, echo=TRUE, message=FALSE, warning=FALSE}
cvfit0$lambda.min
coef(cvfit0, s = "lambda.min")
```

### lambda.lse

* Unadjusted Cross-Validated Cox Regression Coefficients using `lambda.lse`
* `lambda.min` is the largest value of lambda such that error is within 1 standard error of the minimum.
# Cross-validated coefficients of the Cox Regression Model using the lambda.lse

```{r, Unadjusted-M0-cv-lse, echo=TRUE, message=FALSE, warning=FALSE}
cvfit0$lambda.1se
coef(cvfit0)  # By default s = "lambda.min"
```

# glmnetUtils (M0)

* Glmnet models fitted for multiple alpha 
* We use cross-validation to tune hyperparameter $\alpha$.
* The idea of "explicitly control the fold" is implemented in `glmnetUtils` package
* The `cva.glmnet` function does simultaneous cross-validation for both the $\alpha$ and $\lambda$ parameters in an elastic net model.

source: (https://glmnet.stanford.edu/articles/glmnet.html)

```{r glmnetUtils-M0}

set.seed(46)
alphv <- seq(0, 1, len = 11)^3
cva_pfit0 <- cva.glmnet(x=x0,y=ySurv0,family = "cox", alpha = alphv)
minlossplot(cva_pfit0)
```

Extract the optimal (hyper)parameters from `cva.glmnet` object

```{r get_best_params-M0}
# Extract the best parameters from cva.glmnet object.
get_model_params <- function(fit) {
  alpha <- fit$alpha
  lambdaMin <- sapply(fit$modlist, `[[`, "lambda.min")
  lambdaSE <- sapply(fit$modlist, `[[`, "lambda.1se")
  error <- sapply(fit$modlist, function(mod) {min(mod$cvm)})
  best <- which.min(error)
  data.frame(alpha = alpha[best], lambdaMin = lambdaMin[best],
             lambdaSE = lambdaSE[best], eror = error[best])
}

get_model_params(cva_pfit0)
a0_opt <- get_model_params(cva_pfit0)[["alpha"]]
lmbda0_opt <- get_model_params(cva_pfit0)[["lambdaMin"]]
```

Cox regression coefficients for Model M1 using the optimal combination of (hyper)parameters

```{r get_best_coef-M0}
coef(cva_pfit0, s = lmbda0_opt, alpha = a0_opt)
```

Save `cva_pfit0` object for post-processing

```{r save-cva_pfit0}
save(cva_pfit0, file = "./save/21Cox-Reg1-save0.Rdata")
```


???-M0

# glmnet for Model M1 ($\alpha=0$)


## Data for M1

Data are prepared for Cox regression (Model M1).

```{r create-data-drop-na1}
vars1 <- c(surv_vars, prot_npx)
 
data1 <- scrambled  %>% select(all_of(vars1)) 

#drop any records with NA
data_drop_na1 <- data1 %>% drop_na()    

# Total sample size and number of columns
dim(data_drop_na1)

#includes all proteins after excluding missing data
x1 <- data.matrix(data_drop_na1[, prot_npx])
dim(x1)
colnames(x1)  # 21 proteins

# time and status
tx <- data.matrix(data_drop_na1[,c("time","status")])
ty    <- tx[, "time"]
stx <- tx[,"status"]
ySurv1 <- survival::Surv(ty, stx)   
table(data_drop_na1$status) # 0-censored, 1-observed time-to-ESKD
```

## Model M1 fit $\alpha=0$

* By default mixing (hyper) parameter $\alpha$ is equal to $1$ (lasso)
* In this example we arbitrarily assume $\alpha$ is equal to $0$ (ridge regression)


```{r Unadjusted, echo =TRUE, message=FALSE, warning=FALSE}
#fit the unadjusted cox regression model
pfit1 <- glmnet(x1, ySurv1, family = "cox", alpha=0)
class(pfit1)
```
          
## Extracting info from Model M1 fit

...

###  Model M1 Coefficients Profile Plots

* (Standardized) coefficients plotted versus ${\ell}_1$-norm (using `plot()` function:

```{r, coefficient-profile-plot1, echo=TRUE, message=FALSE}
plot(pfit1, label = TRUE) 
```

"Each curve corresponds to a variable. It shows the path of its coefficient 
against the ${\ell}_1$-norm of the whole 
coefficient vector as $\lambda$ varies. The axis above indicates 
the number of nonzero coefficients at the current $\lambda$,
which is the effective degrees of freedom (df) for the lasso."

source: ("https://glmnet.stanford.edu/articles/glmnet.html")

* Dynamic plot of (standardized) coefficients plotted versus $\log(\lambda)$ using `coefplot::coefpath`:

```{r, coefficient-profile-plot12, echo=TRUE, message=FALSE, warning=FALSE}
coefpath(pfit1) 
```

# cv.glmnet for M1 ($\alpha=0$)

## C-V for M1 ($\alpha=0$)

* Cross-Validated Cox Regression Model Coefficient Plot for Model M1

```{r, Unadjusted_cv_plot, echo=TRUE, message=FALSE}
set.seed(111)
#fit the cross-validated model
cvfit1 <- cv.glmnet(x1, ySurv1, family = "cox", alpha=0)
plot(cvfit1, label = TRUE) 
```

"This plots the cross-validation curve (red dotted line) along with upper and lower standard deviation curves
along the $\lambda$ sequence (error bars). Two special values along the $\lambda$ sequence are indicated by the 
vertical dotted lines. `lambda.min` is the value of $\lambda$ that gives minimum mean cross-validated error, 
while `lambda.1se` is the value of $\lambda$ that gives the most regularized model such that the cross-validated 
error is within one standard error of the minimum."

source: ("https://glmnet.stanford.edu/articles/glmnet.html")

## Extract info from C-V (M1)

### minimal lambda value

* Unadjusted Cross-Validated Cox Regression Coefficients using `lambda.min` value 
* get the minimal lambda value (value of lambda that gives minimum cvm)

```{r, Unadjusted_cv_min, echo=TRUE, message=FALSE, warning=FALSE}
cvfit1$lambda.min
coef(cvfit1, s = "lambda.min")
```

### lambda.lse

* Unadjusted Cross-Validated Cox Regression Coefficients using `lambda.lse`
* `lambda.min` is the largest value of lambda such that error is within 1 standard error of the minimum.
# Cross-validated coefficients of the Cox Regression Model using the lambda.lse

```{r, Unadjusted_cv_lse, echo=TRUE, message=FALSE, warning=FALSE}
cvfit1$lambda.1se
coef(cvfit1)  # By default s = "lambda.min"
```

# glmnetUtils (M1)

* Glmnet models fitted for multiple alpha 
* We use cross-validation to tune hyperparameter $\alpha$.
* The idea of "explicitly control the fold" is implemented in `glmnetUtils` package
* The `cva.glmnet` function does simultaneous cross-validation for both the $\alpha$ and $\lambda$ parameters in an elastic net model.

source: (https://glmnet.stanford.edu/articles/glmnet.html)

```{r glmnetUtils}

set.seed(46)
alphv <- seq(0, 1, len = 11)^3
cva_pfit1 <- cva.glmnet(x=x1,y=ySurv1,family = "cox", alpha = alphv)
minlossplot(cva_pfit1)
```

Extract the optimal (hyper)parameters from `cva.glmnet` object

```{r get_best_params}
# Extract the best parameters from cva.glmnet object.
get_model_params <- function(fit) {
  alpha <- fit$alpha
  lambdaMin <- sapply(fit$modlist, `[[`, "lambda.min")
  lambdaSE <- sapply(fit$modlist, `[[`, "lambda.1se")
  error <- sapply(fit$modlist, function(mod) {min(mod$cvm)})
  best <- which.min(error)
  data.frame(alpha = alpha[best], lambdaMin = lambdaMin[best],
             lambdaSE = lambdaSE[best], eror = error[best])
}

get_model_params(cva_pfit1)
a1_opt <- get_model_params(cva_pfit1)[["alpha"]]
lmbda1_opt <- get_model_params(cva_pfit1)[["lambdaMin"]]
```

Cox regression coefficients for Model M1 using the optimal combination of (hyper)parameters

```{r get_best_coef}
coef(cva_pfit1, s = lmbda1_opt, alpha = a1_opt)
```

Save `cva_pfit1` object for post-processing

```{r save-cva_pfit1}
save(cva_pfit1, file = "./save/21Cox-Reg1-save1.Rdata")
```


# glmnet for Model M2 ($\alpha=0$)


* Adjusted Cox Regression Model M2(adjusting for Baseline HbA1c, Log10(ACR), eGFR, Sex, and Age)

## Data preparation

```{r, Adjusted, echo=TRUE, message=FALSE, warning=FALSE}

vars2 <- c(surv_vars,  prot_npx, clin_vars)
 
data2 <- scrambled  %>% select(all_of(vars2)) 

#drop any records with NA
dim(data2)         

#drop any records with NA
data_drop_na2 <- data2 %>% drop_na()    

#Total sample size
nrow(data_drop_na2)

#includes all proteins after excluding missing data
x2 <- data.matrix(data_drop_na2[,c(prot_npx, clin_vars)])

#select follow-up time and status
y2 <- data.matrix(data_drop_na2[,c("time","status")])
y2Surv <- survival::Surv(y2[,"time"], y2[,"status"])
```

## Model M2 fit ($\alpha=0$)

```{r glmnet-pfit2, echo=TRUE, message=FALSE, warning=FALSE}
#fit the adjusted cox regression model
pfit2 <- glmnet(x2, y2Surv, family = "cox", alpha=0)

```

## Extracting info from Model M2

### Coefficient plots

* Adjusted Cox Regression Model Coefficient Plot

```{r, Adjusted_plot, echo=TRUE, message=FALSE, warning=FALSE}
plot(pfit2, label = TRUE) 
```



# cv.glmnet for M2 ($\alpha=0$)


## C-V for M2 ($\alpha=0$)

* Adjusted Cross-Validated Cox Regression Model Coefficient Plot

```{r, Adjusted_cv_plot, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(222)
#fit the cross-validatedmodel
cvfit2 <- cv.glmnet(x2, y2Surv, family = "cox", alpha=0)

plot(cvfit2, label = TRUE) 
```


## Extract info from C-V (M2)
### Coefficients for selected $\lambda$

* Adjusted Cross-Validated Cox Regressionn Coefficients using `lambda.min` value

```{r, Adjusted_cv_min, echo=TRUE, message=FALSE, warning=FALSE}
#get the minimal lambda value (value of lambda that gives minimum cvm)
(l_min <- cvfit2$lambda.min)

# Express on natural log scale
log(l_min)
# Cross-validated Cox Regression Model using the minimal lambda
coef(cvfit2, s = "lambda.min")
```

* Adjusted Cross-Validated Cox Regression Coefficients using `lambda.1se`
* largest value of lambda such that error is within 1 standard error of the minimum.

```{r, Adjusted_cv_lse, echo=TRUE, message=FALSE, warning=FALSE}
(l_1se <- cvfit2$lambda.1se)

# Express on natural log scale
log(l_1se)

coef(cvfit2)
```

# glmnetUtils (M2)

* Glmnet models for multiple alpha 
* We use cross-validation to tune hyperparameter $\alpha$.
* The idea of "explicitly control the fold" is implemented in `glmnetUtils` package
* The cva.glmnet function does simultaneous cross-validation for both the $\alpha$ and $\lambda$ parameters in an elastic net model.

source: (https://glmnet.stanford.edu/articles/glmnet.html)

```{r glmnetUtils_M2}

set.seed(46)
alphv <- seq(0, 1, len = 11)^3
cva_pfit2 <- cva.glmnet(x=x2,y=y2Surv,family = "cox", alpha = alphv)
minlossplot(cva_pfit2)
```

Extract optimal (hyper)parameters from `cva.glmnet` object

```{r get_best_params_M2}

(pfit2_params <- get_model_params(cva_pfit2))

a2_opt <- pfit2_params[["alpha"]]
lmbda2_opt <- pfit2_params[["lambdaMin"]]
```

Cox regression coefficients for Model M2 using the best combination of (hyper)parameters

```{r get_best_coef_M2}

coef(cva_pfit2, s = lmbda2_opt, alpha = a2_opt)
```
## Plotting survival curves for optimal M2 model

See examples at: https://glmnet.stanford.edu/reference/survfit.coxnet.html

Notes: 

* `survfit` computes the predicted survivor function for a Cox PH model with elastic net penalty.
* the design matrix x and response ySurv used to fit the model need to be passed to `survfit` function

* Step 1: Fit the glmnet model to original data (stored in x2 and y2Surv objects) using optimal alpha.
* Note: Resulting object `pfit2_aopt` contains models for multiple lambdas. 

```{r pfit2_a-object}
pfit2_aopt <- glmnet(x2, y2Surv, family = "cox", alpha= a2_opt)
```

```{r surv2-plot}
# survfit object for penalized Cox model
sf0 <- survival::survfit(pfit2_aopt, s = lmbda2_opt, x = x2, y = y2Surv)
plot(sf0)
```

Note that the same plot can be obtained using the code below.

```{r surv-plot-mean}
x2mean <- apply(x2, 2, mean)
sfm <- survival::survfit(pfit2_aopt, s = lmbda2_opt, x = x2, y = y2Surv, newx = x2mean)
# plot(sfm)
```


# Performance of optimal model M2

This section should be considered as an illustration only, because the `glmnet` model
performance is assessed using training data. For this reason performance of the model
is too optimistic.


## Preparatory steps

* Step 1.  Object `pfit2_aopt` contains models for optimal alpha. It was created earlier in this document 
It contains models for multiple lambdas. 


Step 2: Prepare test data

```{r Prepare-test-data-M2}
dim(data_drop_na2)
test_rows <- 1:nrow(x2)    # Select rows for testing. Possibly all rows in data_drop_na2.
x2_test <- x2[test_rows,]
y2_test <- y2[test_rows,]
y2Surv_test <- survival::Surv(y2_test[,"time"], y2_test[,"status"] )

data2_test <- data_drop_na2[test_rows, ]
range(data2_test$time)
```

Step 3: Calculate predicted values for test data

```{r calc-pred2-vals}
predM2_lpmtx <- predict(pfit2_aopt, newx = x2_test, type = "link") # Matrix
predM2_lp <- as.vector(predict(pfit2_aopt, newx = x2_test, type = "link", s = lmbda2_opt))
summary(predM2_lp)
```

## Predictive performance of optimal M2

### C-index

Ref: Harrel Jr, F. E. and Lee, K. L. and Mark, D. B. (1996) Tutorial in biostatistics:
multivariable prognostic models: issues in developing models, evaluating assumptions 
and adequacy, and measuring and reducing error, _Statistics in Medicine_, 15, pages 361-387.


```{r C-index-M2}
apply(predM2_lpmtx, 2, Cindex, y = y2Surv_test) # Multiple lambda
Cindex(predM2_lp, y2Surv_test)                  # Optimal lambda
```

### Time-dependent ROC

* source: (https://datascienceplus.com/time-dependent-roc-for-survival-prediction-models-in-r)



```{r surv-roc}
                
## Augment `data2_test` with linear predictor
 data2_test$predM2_lp <- predM2_lp

# Evaluate every 2.5 years
 
tx <- 2.5* c(2,3,4,5,6) # ROC time points

survROC_lp2 <- create_survivalROC_data(tx, data2_test, predM2_lp)
 
# survivalROC_data %>% print (n=100)
```

* Plot Time-dependent ROC every 2.5 years

```{r surv-roc-plot}
## Plot Time-dependent ROC every 2.5 years
plot_timedep_ROC(survROC_lp2) 
```


* Save `cva_pfit2` object for post-processing

```{r save-cva_pfit2_M2}
save(cva_pfit2, file = "./save/21Cox-Reg1-save2.Rdata")
```

# Fit optimal models using tidymodels

Based on (https://parsnip.tidymodels.org/reference/details_proportional_hazards_glmnet.html)

```{r tidy-libs}
library(survival)
library(censored)
library(dplyr)
library(tidyr)
```


## Model M0

Create `mod0_all` object.

```{r tidy-M0-all}
# M0 for all alpha
mod0_all <- map(alphv, function(a){
  proportional_hazards(penalty = tune(), mixture = a) %>% 
  set_engine("glmnet") %>% 
  fit(Surv(time, status) ~ ., data = data_drop_na0)
})
length(mod0_all)
tmp <- mod0_all[[2]]
class(tmp)

```

* Create `mod0_opt` object

```{r tidy-M0-opt}
mod0_opt <- 
  proportional_hazards(penalty = lmbda0_opt, mixture = a0_opt) %>% 
  set_engine("glmnet") %>% 
  fit(Surv(time, status) ~ ., data = data_drop_na0)
class(mod0_opt)
```
??? M0-end

## Model M1

Create `mod1_all` object.

```{r tidy-M1-all}
# M1 for all alpha
mod1_all <- map(alphv, function(a){
  proportional_hazards(penalty = tune(), mixture = a) %>% 
  set_engine("glmnet") %>% 
  fit(Surv(time, status) ~ ., data = data_drop_na1)
})
length(mod1_all)
tmp <- mod1_all[[2]]
class(tmp)

```
* Create `mod1_opt` object

```{r tidy-M1-opt}
mod1_opt <- 
  proportional_hazards(penalty = lmbda1_opt, mixture = a1_opt) %>% 
  set_engine("glmnet") %>% 
  fit(Surv(time, status) ~ ., data = data_drop_na1)
class(mod1_opt)
```

## Model M2

Create `mod2_all` object.

```{r tidy-M2-all}
# M2 for all alpha
mod2_all <- map(alphv, function(a){
  proportional_hazards(penalty = tune(), mixture = a) %>% 
  set_engine("glmnet") %>% 
  fit(Surv(time, status) ~ ., data = data_drop_na2)
})
length(mod2_all)
tmp <- mod2_all[[2]]
class(tmp)
```
* Create `mod2_opt` object

```{r tidy-M2-opt}
mod2_opt <- 
  proportional_hazards(penalty = lmbda2_opt, mixture = a2_opt) %>% 
  set_engine("glmnet") %>% 
  fit(Surv(time, status) ~ ., data = data_drop_na2)
class(mod2_opt)
```

* Calculate predicted survival probabilities

```{r tidy-survprob-M2}
# Predicted survival probabilities for different subjects
pred_surv <- predict(mod2_opt, data2_test, type = "survival", time = c(5,10)) %>% 
  # bind_cols(data2_test) %>% 
  unnest(.pred)
colnames(pred_surv)
```



* Predict survival time

```{r tidy-survtime-M2}
colnames(data2_test)

pred_M2tidy <- predict(mod2_opt, data2_test, type = "time") %>% 
 cbind(data2_test) %>%
 select(.pred_time, time, status, predM2_lp) %>%
 rename(predM2_time = .pred_time) %>% as_tibble()
 
pred_M2tidy %>% print(n=30)
plot(pred_M2tidy$predM2_time, pred_M2tidy$time)

```


* save tidy objects

```{r save-tidy}
save(mod0_all, mod0_opt, mod1_all, mod1_opt, mod2_opt, mod2_all, file ="./save/3Coxreg1-tidy-save.Rdata")
```

